# ğŸ”¥ OptimizaciÃ³n en PyTorch: ExplicaciÃ³n de los principales optimizadores  

En este documento, se explican los diferentes optimizadores seleccionables en el **torch_template**, junto con sus parÃ¡metros clave y cÃ³mo ajustarlos (con valores recomendados) para mejorar el entrenamiento de modelos de **Deep Learning**.

---

## ğŸŸ¢ 1. Adam (Adaptive Moment Estimation)  

El optimizador **Adam** combina las ventajas de **SGD con momentum** y **RMSprop**, adaptando la tasa de aprendizaje de cada parÃ¡metro de manera independiente.

```python
optimizer = torch.optim.Adam(
    model.parameters(), 
    lr=0.001,
    weight_decay=0, 
)
```

### ğŸ”¹ **ParÃ¡metros de Adam**  
- **`lr` (Learning Rate, por defecto `0.001`)**  
  - Controla la magnitud de los pasos en la direcciÃ³n del gradiente.  
  - Valores recomendados: `0.0001 - 0.001`  
- **`weight_decay` (default=`0`)**  
  - AÃ±ade regularizaciÃ³n L2, pero **AdamW es mejor alternativa**.   

---

## ğŸ”µ 2. AdamW (Adam con mejor regularizaciÃ³n L2)  

Corrige el problema de **weight decay** en Adam, evitando que se aplique incorrectamente.

```python
optimizer = torch.optim.AdamW(
    model.parameters(), 
    lr=0.001,
    weight_decay=0
)
```

âœ… **Recomendado si quieres evitar sobreajuste con regularizaciÃ³n L2.**

---

## ğŸŸ  3. SGD (Stochastic Gradient Descent) con Momentum  

El **descenso de gradiente estocÃ¡stico (SGD)** es mÃ¡s simple y generalmente ofrece mejor **generalizaciÃ³n** que Adam.

```python
optimizer = torch.optim.SGD(
    model.parameters(), 
    lr=0.01, 
    momentum=0.9, 
    weight_decay=0.0001, 
)
```

### ğŸ”¹ **ParÃ¡metros de SGD**  
- **`lr`**: Define el tamaÃ±o de los pasos en la actualizaciÃ³n de pesos.  
- **`momentum` (default=`0.9`)**:  
  - Reduce la variabilidad en la direcciÃ³n del gradiente. (Configurable solo desde linea de cÃ³digo)  
  - Valores recomendados: `0.9`  
- **`weight_decay`**: RegularizaciÃ³n L2 para evitar sobreajuste.   

---

## ğŸ”´ 4. RMSprop (Root Mean Square Propagation)  

RMSprop es Ãºtil para problemas donde los gradientes oscilan mucho, como **redes recurrentes**.

```python
optimizer = torch.optim.RMSprop(
    model.parameters(), 
    lr=0.001, 
    alpha=0.99, 
    eps=1e-8, 
    weight_decay=0, 
    momentum=0.9
)
```

### ğŸ”¹ **ParÃ¡metros de RMSprop**  
- **`lr`**: Tasa de aprendizaje.  
- **`alpha` (default=`0.99`)**:  
  - Define cuÃ¡nto contribuyen los gradientes antiguos en el nuevo ajuste. (Configurable solo desde linea de cÃ³digo)  
  - Valores recomendados: `0.9 - 0.99`  
- **`eps`**: TÃ©rmino de estabilidad numÃ©rica. (Configurable solo desde linea de cÃ³digo)  
- **`weight_decay`**: RegularizaciÃ³n L2.  
- **`momentum`**: Permite que el optimizador almacene informaciÃ³n de los gradientes pasados. (Configurable solo desde linea de cÃ³digo)   

âœ… **RMSprop suele funcionar mejor que Adam en problemas con gradientes muy oscilantes.**

---

## ğŸŸ£ 5. Adagrad (Adaptive Gradient Algorithm)  

Adagrad ajusta la tasa de aprendizaje para cada parÃ¡metro de forma individual, Ãºtil en **datasets con caracterÃ­sticas raras**.

```python
optimizer = torch.optim.Adagrad(
    model.parameters(), 
    lr=0.01, 
    lr_decay=0, 
    weight_decay=0
)
```

### ğŸ”¹ **ParÃ¡metros de Adagrad**  
- **`lr`**: Define el tamaÃ±o inicial del paso de actualizaciÃ³n.  
- **`lr_decay` (default=`0`)**:  
  - Controla cÃ³mo decae el learning rate con el tiempo. (Configurable solo desde linea de cÃ³digo) 
- **`weight_decay`**: RegularizaciÃ³n L2.  

ğŸš¨ **Problema:** Reduce demasiado el `lr` con el tiempo, lo que puede hacer que el entrenamiento se detenga prematuramente.

---

## ğŸŸ¤ 6. Adadelta (Mejora de Adagrad)  

Elimina la necesidad de definir un **learning rate**, adaptÃ¡ndolo automÃ¡ticamente.

```python
optimizer = torch.optim.Adadelta(
    model.parameters(), 
    rho=0.9, 
    eps=1e-6
)
```

### ğŸ”¹ **ParÃ¡metros de Adadelta**  
- **`rho` (default=`0.9`)**:  
  - Controla la memoria del gradiente pasado. (Configurable solo desde linea de cÃ³digo) 
- **`eps`**: TÃ©rmino de estabilidad numÃ©rica. (Configurable solo desde linea de cÃ³digo) 

âœ… **Ãštil si no quieres preocuparte por el ajuste del `lr`.**

---



---

# ğŸ“Š ComparaciÃ³n rÃ¡pida  

| Optimizador | Velocidad | GeneralizaciÃ³n | Estabilidad | CuÃ¡ndo usarlo |
|-------------|-----------|----------------|-------------|---------------|
| **Adam** | ğŸ”¹ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ğŸ”¹ | Casos generales |
| **AdamW** | ğŸ”¹ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ğŸ”¹ | Evitar sobreajuste |
| **SGD+Momentum** | ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ | Modelos grandes |
| **RMSprop** | ğŸ”¹ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ | RNNs, gradientes oscilantes |
| **Adagrad** | ğŸ”¹ | ğŸ”¹ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ğŸ”¹ | NLP, datos dispersos |
| **Adadelta** | ğŸ”¹ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ğŸ”¹ | ğŸ”¹ğŸ”¹ğŸ”¹ | Evitar ajuste del LR |

---

# ğŸš€ Resumen 

- Si **Adam te funciona bien**, prueba **AdamW** para mejor regularizaciÃ³n.  
- Si **el modelo sobreajusta**, usa **SGD con momentum**.  
- Si **los gradientes oscilan mucho**, usa **RMSprop**.   

ğŸ“Œ **Â¿CuÃ¡l de estos quieres probar en tu caso?** ğŸ˜ƒ