# ğŸ† **Funciones de PÃ©rdida para ClasificaciÃ³n en CNNs**  

Cuando entrenamos una **red convolucional** para tareas de clasificaciÃ³n, elegir la funciÃ³n de pÃ©rdida correcta es clave para mejorar la precisiÃ³n del modelo.  

A continuaciÃ³n, se presentan las mejores funciones de pÃ©rdida para clasificaciÃ³n, dependiendo de si el problema es **binario, multiclase o desbalanceado**.

---

## ğŸŸ¢ **1. CrossEntropyLoss (ClasificaciÃ³n Multiclase)**  

```python
criterion = nn.CrossEntropyLoss(weight=None, reduction='mean')
```

âœ… **Â¿CuÃ¡ndo usarla?**  
âœ” Cuando hay **mÃ¡s de dos clases** (Ejemplo: clasificaciÃ³n de imÃ¡genes con 10 categorÃ­as).  
âœ” Es la opciÃ³n mÃ¡s comÃºn en tareas de clasificaciÃ³n general.  

ğŸ” **Detalles:**  
- No necesita `softmax`, ya que PyTorch lo aplica internamente.  
- Si las clases estÃ¡n desbalanceadas, se puede usar `weight` para asignar pesos a cada clase.  

---

## ğŸ”µ **2. BCEWithLogitsLoss (ClasificaciÃ³n Binaria)**  

```python
criterion = nn.BCEWithLogitsLoss(pos_weight=None, reduction='mean')
```

âœ… **Â¿CuÃ¡ndo usarla?**  
âœ” Cuando el problema es **binario** (Ejemplo: detectar si hay un objeto en la imagen o no).  
âœ” Se usa cuando la **Ãºltima capa del modelo tiene una Ãºnica neurona**.  

ğŸ” **Detalles:**  
- No necesita `sigmoid`, ya que PyTorch lo aplica internamente.  
- Se puede usar `pos_weight` para ajustar datasets desbalanceados.  

---

## ğŸŸ  **3. Focal Loss (Para Datos Desbalanceados)**  

ğŸ“Œ **Nota:** PyTorch no la tiene implementada por defecto, pero la hemos definido como una clase personalizada.

```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss
        return F_loss.mean() if self.reduction == 'mean' else F_loss.sum()
```

âœ… **Â¿CuÃ¡ndo usarla?**  
âœ” Cuando el dataset estÃ¡ **muy desbalanceado** (Ejemplo: clasificaciÃ³n de imÃ¡genes con muy pocos ejemplos de una clase).  
âœ” Reduce la influencia de las clases mayoritarias y enfatiza las clases minoritarias.  

ğŸ” **Detalles:**  
- `gamma`: Controla cuÃ¡nto se reduce la penalizaciÃ³n para clases mayoritarias.  
- `alpha`: Peso para la clase minoritaria.  

ğŸ“Œ **Ejemplo en CNNs:**  

```python
criterion = FocalLoss(alpha=0.25, gamma=2.0)
```

---

## ğŸ“Š **ComparaciÃ³n de las Mejores Funciones de PÃ©rdida en ClasificaciÃ³n**  

| FunciÃ³n de PÃ©rdida | Tipo de ClasificaciÃ³n | Â¿CuÃ¡ndo Usarla? |
|--------------------|----------------------|----------------|
| **CrossEntropyLoss** | Multiclase | Cuando hay mÃ¡s de dos clases |
| **BCEWithLogitsLoss** | Binaria | Cuando solo hay dos clases |
| **Focal Loss** | Binaria o Multiclase | Cuando hay un gran desbalance de clases |

---

## ğŸš€ **Â¿CuÃ¡l elegir para tu CNN?**  

âœ” **Si tu CNN clasifica en mÃ¡s de dos clases â†’** `CrossEntropyLoss`  
âœ” **Si tu CNN clasifica en dos clases â†’** `BCEWithLogitsLoss`  
âœ” **Si tu dataset estÃ¡ muy desbalanceado â†’** `Focal Loss`  

---

